{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxntxfRvhsUJ"
      },
      "source": [
        "# **CK+**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0N-E-knxhqaG"
      },
      "outputs": [],
      "source": [
        "import pylab as pl\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mplib \n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "from sklearn.svm import SVC\n",
        "import joblib\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePsefZQah3OU",
        "outputId": "a0fc819c-ad03-46c0-8119-90c59730c6d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded the images of dataset-anger\n",
            "\n",
            "Loaded the images of dataset-contempt\n",
            "\n",
            "Loaded the images of dataset-disgust\n",
            "\n",
            "Loaded the images of dataset-fear\n",
            "\n",
            "Loaded the images of dataset-happy\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded the images of dataset-sadness\n",
            "\n",
            "Loaded the images of dataset-surprise\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(945, 2304)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_path = 'F:\\ckextended\\CK+48'\n",
        "data_dir_list = os.listdir(data_path)\n",
        "\n",
        "img_data_list = []\n",
        "img_data = []\n",
        "num_images = []\n",
        "\n",
        "for dataset in ['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']:\n",
        "    img_list = os.listdir(data_path + '/' + dataset)\n",
        "    print('Loaded the images of dataset-' + '{}\\n'.format(dataset))\n",
        "    num_images.append(len(img_list))\n",
        "    for img in img_list:\n",
        "        input_img = cv2.imread(data_path + '/' + dataset + '/' + img, 0)  # Read image in grayscale\n",
        "        img_data_list.append(input_img)\n",
        "\n",
        "img_data = np.array(img_data_list)\n",
        "img_data = img_data.astype('float32')\n",
        "img_data = img_data / 255\n",
        "\n",
        "n_samples, h, w = img_data.shape\n",
        "np.random.seed(42)\n",
        "\n",
        "img_data = img_data.reshape(n_samples, -1)\n",
        "\n",
        "img_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "BtSpkjKlh_Z5"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_classes = 7\n",
        "\n",
        "num_of_samples = img_data.shape[0]\n",
        "labels = np.ones((num_of_samples,), dtype='int64')\n",
        "\n",
        "labels[0:45] = 0  # 45\n",
        "labels[45:63] = 1  # 18\n",
        "labels[63:122] = 2  # 59\n",
        "labels[122:147] = 3  # 25\n",
        "labels[147:216] = 4  # 69\n",
        "labels[216:244] = 5  # 28\n",
        "labels[244:327] = 6  # 83\n",
        "\n",
        "names = ['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise']\n",
        "\n",
        "n_classes = len(names)\n",
        "\n",
        "def getLabel(id):\n",
        "    return ['anger', 'contempt', 'disgust', 'fear', 'happy', 'sadness', 'surprise'][id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2tHYfQ3iOOG",
        "outputId": "0aa4dcc4-5a55-4aa7-fa81-75f6d5d4f2b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((756, 2304), (756,), (189, 2304), (189,))"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y = to_categorical(labels, num_classes)\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(img_data, y, test_size=0.20, shuffle=True, random_state=42)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract HOG features\n",
        "def extract_hog_features(images):\n",
        "    hog_features = []\n",
        "    for img in images:\n",
        "        fd, hog_image = hog(img.reshape((h, w)), orientations=8, pixels_per_cell=(4, 4),\n",
        "                            cells_per_block=(1, 1), visualize=True)\n",
        "        hog_features.append(fd)\n",
        "    return np.array(hog_features)\n",
        "\n",
        "X_train_hog = extract_hog_features(X_train)\n",
        "X_test_hog = extract_hog_features(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMe72QdBjPDj",
        "outputId": "d4e552aa-075e-49b3-a489-ffc683c4e782"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['pca.joblib']"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# PCA\n",
        "n_components = 120\n",
        "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train_hog)\n",
        "X_train_pca = pca.transform(X_train_hog)\n",
        "X_test_pca = pca.transform(X_test_hog)\n",
        "\n",
        "joblib.dump(pca, 'pca.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odVRB7oVpMld",
        "outputId": "25474e4c-567c-4506-8db4-ceaede651361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting the classifier to the training set\n",
            "Best estimator found by grid search:\n",
            "SVC(C=5000.0, class_weight='balanced', gamma=5e-05, kernel='sigmoid')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['svm_model.joblib']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# SVM\n",
        "print(\"Fitting the classifier to the training set\")\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 1e1, 5e1, 1e2, 5e2, 1e3, 5e3, 1e4, 5e4, 1e5, 5e5, 1e6],\n",
        "    'gamma': [1e-8, 5e-8, 1e-7, 5e-7, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2],\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(SVC(kernel='sigmoid', class_weight='balanced'), param_grid)\n",
        "clf = clf.fit(X_train_pca, y_train)\n",
        "\n",
        "print(\"Best estimator found by grid search:\")\n",
        "best_estimator = clf.best_estimator_\n",
        "print(clf.best_estimator_)\n",
        "joblib.dump(clf, 'svm_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YEc8LZdpYb2",
        "outputId": "11da0b30-91a9-47a5-8433-09068756703c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting the people names on the testing set\n"
          ]
        }
      ],
      "source": [
        "print(\"Predicting the people names on the testing set\")\n",
        "y_pred = best_estimator.predict(X_test_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKwo9JcFpYb3",
        "outputId": "b919475a-135b-4b76-bf1a-83954e597f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       anger       1.00      1.00      1.00         6\n",
            "    contempt       0.97      1.00      0.98       126\n",
            "     disgust       1.00      0.93      0.97        15\n",
            "        fear       0.80      1.00      0.89         4\n",
            "       happy       1.00      0.83      0.91        12\n",
            "     sadness       1.00      1.00      1.00         2\n",
            "    surprise       1.00      0.92      0.96        24\n",
            "\n",
            "    accuracy                           0.97       189\n",
            "   macro avg       0.97      0.95      0.96       189\n",
            "weighted avg       0.98      0.97      0.97       189\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred, target_names=names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMWaeVxVpYb3",
        "outputId": "3b946f8a-87c5-405f-9d0d-42974bdc9baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  6   0   0   0   0   0   0]\n",
            " [  0 126   0   0   0   0   0]\n",
            " [  0   0  14   1   0   0   0]\n",
            " [  0   0   0   4   0   0   0]\n",
            " [  0   2   0   0  10   0   0]\n",
            " [  0   0   0   0   0   2   0]\n",
            " [  0   2   0   0   0   0  22]]\n"
          ]
        }
      ],
      "source": [
        "print(confusion_matrix(y_test, y_pred, labels=range(num_classes)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLAv30KepYb3"
      },
      "outputs": [],
      "source": [
        "def title(y_pred, y_test, target_names, i):\n",
        "    pred_name = names[y_pred[i]].rsplit(' ', 1)[-1]\n",
        "    true_name = names[y_test[i]].rsplit(' ', 1)[-1]\n",
        "    return 'predicted: %s\\ntrue: %s' % (pred_name, true_name)\n",
        "\n",
        "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
        "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
        "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
        "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
        "    for i in range(n_row * n_col):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
        "        plt.title(titles[i], size=12)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 2304 into shape (233,233)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32me:\\Download\\emotion-detection-main\\pca_svm_model.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         plt\u001b[39m.\u001b[39myticks(())\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Now you can call the function with the correct dimensions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plot_gallery(X_test, prediction_titles, h, w)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
            "\u001b[1;32me:\\Download\\emotion-detection-main\\pca_svm_model.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_row \u001b[39m*\u001b[39m n_col):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     plt\u001b[39m.\u001b[39msubplot(n_row, n_col, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     plt\u001b[39m.\u001b[39mimshow(images[i]\u001b[39m.\u001b[39mreshape((h, w)), cmap\u001b[39m=\u001b[39mplt\u001b[39m.\u001b[39mcm\u001b[39m.\u001b[39mgray)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     plt\u001b[39m.\u001b[39mtitle(titles[i], size\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Download/emotion-detection-main/pca_svm_model.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     plt\u001b[39m.\u001b[39mxticks(())\n",
            "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 2304 into shape (233,233)"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
        "    \"\"\"Helper function to plot a gallery of images\"\"\"\n",
        "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
        "    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
        "    for i in range(n_row * n_col):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
        "        plt.title(titles[i], size=12)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "\n",
        "# Now you can call the function with the correct dimensions\n",
        "plot_gallery(X_test, prediction_titles, h, w)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OpdOXwdpYb3",
        "outputId": "f8689ef2-0133-4e1f-af27-6844c09c54fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9735449735449735\n"
          ]
        }
      ],
      "source": [
        "from pyexpat import model\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import accuracy_score\n",
        "score = accuracy_score(y_test, y_pred)\n",
        "print(score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bamo-aMvpfP4"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA as RandomizedPCA\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Assuming you have already trained the SVM model and PCA in a separate script\n",
        "\n",
        "# Load the trained SVM model and PCA\n",
        "trained_model = SVC(C=5e4, gamma=1e-5, kernel='sigmoid', class_weight='balanced')\n",
        "trained_model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Assuming pca is already fitted in your training script\n",
        "pca = RandomizedPCA(n_components=120, whiten=True).fit(X_train)\n",
        "\n",
        "# Load the Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Open a connection to the camera (0 is the default camera)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Convert the frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract the face region\n",
        "        face_roi = gray[y:y + h, x:x + w]\n",
        "\n",
        "        # Resize the face region to match the size used during training\n",
        "        face_resized = cv2.resize(face_roi, (64, 64))\n",
        "\n",
        "        # Flatten and normalize the face region\n",
        "        face_flattened = face_resized.flatten() / 255.0\n",
        "\n",
        "        # Ensure the face has the correct number of features (2304) expected by PCA\n",
        "        if len(face_flattened) == 4096:\n",
        "            face_flattened = face_flattened[:2304]\n",
        "\n",
        "        # Transform the face using PCA\n",
        "        face_transformed = pca.transform([face_flattened])\n",
        "\n",
        "        # Make a prediction using the trained model\n",
        "        emotion_prediction = trained_model.predict(face_transformed)\n",
        "\n",
        "        # Get the predicted emotion label\n",
        "        emotion_label = names[emotion_prediction[0]]\n",
        "\n",
        "        # Draw a rectangle around the face and display the emotion label\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    # Display the frame\n",
        "    cv2.imshow('Real-Time Emotion Detection', frame)\n",
        "\n",
        "    # Break the loop when 'q' is pressed\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the camera and close all OpenCV windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "BEST_pca_svm_model.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
